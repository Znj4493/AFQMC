"""
æ¢ç´¢æ€§æ•°æ®åˆ†æ (Exploratory Data Analysis, EDA)

ç›®æ ‡ï¼šæ·±å…¥ç†è§£AFQMCæ•°æ®é›†çš„ç‰¹å¾ï¼Œä¸ºæ¨¡å‹è®¾è®¡æä¾›ä¾æ®
"""

import sys
import os
from pathlib import Path

# è‡ªåŠ¨æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼ˆæ— è®ºä»å“ªé‡Œè¿è¡Œéƒ½èƒ½æ­£ç¡®å¯¼å…¥ï¼‰
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re

# å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„æ•°æ®åŠ è½½æ¨¡å—
from src.data_loader import load_train_data, load_test_data, get_data_statistics

# Set matplotlib font (use English to avoid font issues)
plt.rcParams['axes.unicode_minus'] = False

# Set plot style
sns.set_style("whitegrid")


def analyze_text_length(df: pd.DataFrame):
    """
    åˆ†ææ–‡æœ¬é•¿åº¦åˆ†å¸ƒ

    çŸ¥è¯†ç‚¹ï¼š
    - æ–‡æœ¬é•¿åº¦å½±å“æ¨¡å‹çš„max_lengthå‚æ•°è®¾ç½®
    - äº†è§£é•¿åº¦åˆ†å¸ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¼˜åŒ–paddingç­–ç•¥
    """
    print("\n" + "="*60)
    print("1. æ–‡æœ¬é•¿åº¦åˆ†æ")
    print("="*60)

    # è®¡ç®—æ–‡æœ¬é•¿åº¦
    df['text1_len'] = df['text1'].str.len()
    df['text2_len'] = df['text2'].str.len()
    df['total_len'] = df['text1_len'] + df['text2_len']

    # ç»Ÿè®¡ä¿¡æ¯
    print("\ntext1 é•¿åº¦ç»Ÿè®¡:")
    print(df['text1_len'].describe())

    print("\ntext2 é•¿åº¦ç»Ÿè®¡:")
    print(df['text2_len'].describe())

    print("\næ€»é•¿åº¦ (text1 + text2) ç»Ÿè®¡:")
    print(df['total_len'].describe())

    # å…³é”®å‘ç°
    max_len_95 = df['total_len'].quantile(0.95)
    max_len_99 = df['total_len'].quantile(0.99)

    print(f"\nğŸ’¡ å…³é”®å‘ç°:")
    print(f"  - 95% çš„æ ·æœ¬æ€»é•¿åº¦ <= {max_len_95:.0f} å­—ç¬¦")
    print(f"  - 99% çš„æ ·æœ¬æ€»é•¿åº¦ <= {max_len_99:.0f} å­—ç¬¦")
    print(f"  - å»ºè®®çš„ max_length å‚æ•°: 128 (è¦†ç›–å¤§éƒ¨åˆ†æ ·æœ¬)")

    # Visualization
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # text1 length distribution
    axes[0].hist(df['text1_len'], bins=50, edgecolor='black', alpha=0.7)
    axes[0].set_xlabel('Text Length')
    axes[0].set_ylabel('Count')
    axes[0].set_title('Text1 Length Distribution')
    axes[0].axvline(df['text1_len'].mean(), color='red', linestyle='--', label=f'Mean={df["text1_len"].mean():.1f}')
    axes[0].legend()

    # text2 length distribution
    axes[1].hist(df['text2_len'], bins=50, edgecolor='black', alpha=0.7, color='green')
    axes[1].set_xlabel('Text Length')
    axes[1].set_ylabel('Count')
    axes[1].set_title('Text2 Length Distribution')
    axes[1].axvline(df['text2_len'].mean(), color='red', linestyle='--', label=f'Mean={df["text2_len"].mean():.1f}')
    axes[1].legend()

    # Total length distribution
    axes[2].hist(df['total_len'], bins=50, edgecolor='black', alpha=0.7, color='orange')
    axes[2].set_xlabel('Text Length')
    axes[2].set_ylabel('Count')
    axes[2].set_title('Total Length (text1 + text2)')
    axes[2].axvline(max_len_95, color='red', linestyle='--', label=f'95th={max_len_95:.0f}')
    axes[2].legend()

    plt.tight_layout()
    plt.savefig('results/text_length_distribution.png', dpi=300, bbox_inches='tight')
    print(f"\nâœ… å›¾è¡¨å·²ä¿å­˜è‡³ results/text_length_distribution.png")

    return df


def analyze_label_distribution(df: pd.DataFrame):
    """
    åˆ†ææ ‡ç­¾åˆ†å¸ƒ

    çŸ¥è¯†ç‚¹ï¼š
    - ç±»åˆ«ä¸å¹³è¡¡ä¼šå½±å“æ¨¡å‹è®­ç»ƒ
    - å¯ä»¥é€šè¿‡è°ƒæ•´ç±»åˆ«æƒé‡ã€ä½¿ç”¨Focal Lossç­‰æ–¹æ³•å¤„ç†
    """
    print("\n" + "="*60)
    print("2. æ ‡ç­¾åˆ†å¸ƒåˆ†æ")
    print("="*60)

    label_counts = df['label'].value_counts().sort_index()
    label_ratios = df['label'].value_counts(normalize=True).sort_index()

    print("\næ ‡ç­¾ç»Ÿè®¡:")
    for label in [0, 1]:
        count = label_counts[label]
        ratio = label_ratios[label]
        print(f"  label {label} ({'ä¸ç›¸ä¼¼' if label == 0 else 'ç›¸ä¼¼  '}): {count:5d} ({ratio*100:5.2f}%)")

    # è®¡ç®—ä¸å¹³è¡¡æ¯”ä¾‹
    imbalance_ratio = label_counts[0] / label_counts[1]
    print(f"\nä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f} : 1")

    if imbalance_ratio > 2:
        print(f"âš ï¸  æ•°æ®é›†å­˜åœ¨æ˜¾è‘—çš„ç±»åˆ«ä¸å¹³è¡¡ï¼")
        print(f"   å»ºè®®å¤„ç†æ–¹æ³•:")
        print(f"   1. ä½¿ç”¨ç±»åˆ«æƒé‡ (class_weight)")
        print(f"   2. ä½¿ç”¨ Focal Loss")
        print(f"   3. æ•°æ®é‡é‡‡æ ·ï¼ˆè¿‡é‡‡æ ·/æ¬ é‡‡æ ·ï¼‰")

    # Visualization
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Bar chart
    axes[0].bar([0, 1], label_counts.values, color=['#FF6B6B', '#4ECDC4'], edgecolor='black')
    axes[0].set_xlabel('Label')
    axes[0].set_ylabel('Count')
    axes[0].set_title('Label Distribution (Bar Chart)')
    axes[0].set_xticks([0, 1])
    axes[0].set_xticklabels(['0 (Dissimilar)', '1 (Similar)'])

    # Add value labels
    for i, v in enumerate(label_counts.values):
        axes[0].text(i, v + 500, str(v), ha='center', va='bottom', fontweight='bold')

    # Pie chart
    axes[1].pie(label_counts.values,
                labels=['0 (Dissimilar)', '1 (Similar)'],
                autopct='%1.1f%%',
                colors=['#FF6B6B', '#4ECDC4'],
                startangle=90)
    axes[1].set_title('Label Distribution (Pie Chart)')

    plt.tight_layout()
    plt.savefig('results/label_distribution.png', dpi=300, bbox_inches='tight')
    print(f"\nâœ… å›¾è¡¨å·²ä¿å­˜è‡³ results/label_distribution.png")


def analyze_text_by_label(df: pd.DataFrame):
    """
    å¯¹æ¯”ä¸åŒæ ‡ç­¾çš„æ–‡æœ¬ç‰¹å¾

    ç›®æ ‡ï¼šå‘ç°ç›¸ä¼¼å’Œä¸ç›¸ä¼¼æ–‡æœ¬å¯¹çš„åŒºåˆ«
    """
    print("\n" + "="*60)
    print("3. ä¸åŒæ ‡ç­¾çš„æ–‡æœ¬ç‰¹å¾å¯¹æ¯”")
    print("="*60)

    # åˆ†ç»„ç»Ÿè®¡
    for label in [0, 1]:
        subset = df[df['label'] == label]
        print(f"\nlabel {label} ({'ä¸ç›¸ä¼¼' if label == 0 else 'ç›¸ä¼¼  '}) çš„æ–‡æœ¬é•¿åº¦:")
        print(f"  text1: å¹³å‡ {subset['text1_len'].mean():.1f}, ä¸­ä½æ•° {subset['text1_len'].median():.1f}")
        print(f"  text2: å¹³å‡ {subset['text2_len'].mean():.1f}, ä¸­ä½æ•° {subset['text2_len'].median():.1f}")
        print(f"  æ€»é•¿: å¹³å‡ {subset['total_len'].mean():.1f}, ä¸­ä½æ•° {subset['total_len'].median():.1f}")

    # Visualization comparison
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # text1 length comparison
    df[df['label'] == 0]['text1_len'].hist(bins=30, alpha=0.5, label='label 0', ax=axes[0], color='red')
    df[df['label'] == 1]['text1_len'].hist(bins=30, alpha=0.5, label='label 1', ax=axes[0], color='blue')
    axes[0].set_xlabel('text1 Length')
    axes[0].set_ylabel('Count')
    axes[0].set_title('text1 Length Distribution by Label')
    axes[0].legend()

    # Total length comparison
    df[df['label'] == 0]['total_len'].hist(bins=30, alpha=0.5, label='label 0', ax=axes[1], color='red')
    df[df['label'] == 1]['total_len'].hist(bins=30, alpha=0.5, label='label 1', ax=axes[1], color='blue')
    axes[1].set_xlabel('Total Length')
    axes[1].set_ylabel('Count')
    axes[1].set_title('Total Length Distribution by Label')
    axes[1].legend()

    plt.tight_layout()
    plt.savefig('results/length_by_label.png', dpi=300, bbox_inches='tight')
    print(f"\nâœ… å›¾è¡¨å·²ä¿å­˜è‡³ results/length_by_label.png")


def check_data_quality(df: pd.DataFrame):
    """
    æ•°æ®è´¨é‡æ£€æŸ¥

    æ£€æŸ¥é¡¹ï¼š
    - ç¼ºå¤±å€¼
    - é‡å¤æ ·æœ¬
    - å¼‚å¸¸å€¼
    """
    print("\n" + "="*60)
    print("4. æ•°æ®è´¨é‡æ£€æŸ¥")
    print("="*60)

    # 1. ç¼ºå¤±å€¼æ£€æŸ¥
    print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
    missing = df.isnull().sum()
    print(missing)

    if missing.sum() == 0:
        print("âœ… æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")

    # 2. é‡å¤æ ·æœ¬æ£€æŸ¥
    duplicates = df.duplicated().sum()
    print(f"\né‡å¤æ ·æœ¬æ•°é‡: {duplicates}")

    if duplicates > 0:
        print(f"âš ï¸  å‘ç° {duplicates} æ¡é‡å¤æ ·æœ¬ï¼Œå»ºè®®å»é‡")
    else:
        print("âœ… æ²¡æœ‰å‘ç°é‡å¤æ ·æœ¬")

    # 3. ç©ºæ–‡æœ¬æ£€æŸ¥
    empty_text1 = (df['text1'].str.strip() == '').sum()
    empty_text2 = (df['text2'].str.strip() == '').sum()

    print(f"\nç©ºæ–‡æœ¬ç»Ÿè®¡:")
    print(f"  text1 ä¸ºç©º: {empty_text1} æ¡")
    print(f"  text2 ä¸ºç©º: {empty_text2} æ¡")

    if empty_text1 + empty_text2 == 0:
        print("âœ… æ²¡æœ‰å‘ç°ç©ºæ–‡æœ¬")

    # 4. å¼‚å¸¸é•¿æ–‡æœ¬æ£€æŸ¥
    very_long_threshold = 200
    very_long = df[df['total_len'] > very_long_threshold]

    print(f"\nå¼‚å¸¸é•¿æ–‡æœ¬ (æ€»é•¿åº¦ > {very_long_threshold}):")
    print(f"  æ•°é‡: {len(very_long)} ({len(very_long)/len(df)*100:.2f}%)")

    if len(very_long) > 0:
        print(f"\n  æœ€é•¿çš„3ä¸ªæ ·æœ¬:")
        for idx, row in very_long.nlargest(3, 'total_len').iterrows():
            print(f"    é•¿åº¦={row['total_len']:.0f}, text1={row['text1'][:30]}..., text2={row['text2'][:30]}...")


def sample_analysis(df: pd.DataFrame):
    """
    æ ·æœ¬åˆ†æï¼šå±•ç¤ºå…¸å‹çš„ç›¸ä¼¼å’Œä¸ç›¸ä¼¼æ ·æœ¬
    """
    print("\n" + "="*60)
    print("5. å…¸å‹æ ·æœ¬åˆ†æ")
    print("="*60)

    print("\nã€ç›¸ä¼¼æ ·æœ¬ç¤ºä¾‹ (label=1)ã€‘")
    similar_samples = df[df['label'] == 1].sample(5, random_state=42)
    for i, (idx, row) in enumerate(similar_samples.iterrows(), 1):
        print(f"\næ ·æœ¬ {i}:")
        print(f"  text1: {row['text1']}")
        print(f"  text2: {row['text2']}")

    print("\n\nã€ä¸ç›¸ä¼¼æ ·æœ¬ç¤ºä¾‹ (label=0)ã€‘")
    dissimilar_samples = df[df['label'] == 0].sample(5, random_state=42)
    for i, (idx, row) in enumerate(dissimilar_samples.iterrows(), 1):
        print(f"\næ ·æœ¬ {i}:")
        print(f"  text1: {row['text1']}")
        print(f"  text2: {row['text2']}")


def generate_eda_report(df: pd.DataFrame):
    """
    ç”ŸæˆEDAæ€»ç»“æŠ¥å‘Š
    """
    print("\n" + "="*80)
    print(" "*25 + "EDA æ€»ç»“æŠ¥å‘Š")
    print("="*80)

    stats = get_data_statistics(df)

    print(f"\nğŸ“Š æ•°æ®é›†è§„æ¨¡:")
    print(f"  - æ€»æ ·æœ¬æ•°: {stats['total_samples']:,}")
    print(f"  - Label 0 (ä¸ç›¸ä¼¼): {stats['label_distribution'][0]:,} ({stats['label_0_ratio']*100:.1f}%)")
    print(f"  - Label 1 (ç›¸ä¼¼):   {stats['label_distribution'][1]:,} ({stats['label_1_ratio']*100:.1f}%)")

    print(f"\nğŸ“ æ–‡æœ¬é•¿åº¦ç‰¹å¾:")
    print(f"  - text1 å¹³å‡é•¿åº¦: {df['text1_len'].mean():.1f} å­—ç¬¦")
    print(f"  - text2 å¹³å‡é•¿åº¦: {df['text2_len'].mean():.1f} å­—ç¬¦")
    print(f"  - æ€»é•¿åº¦ 95% åˆ†ä½: {df['total_len'].quantile(0.95):.0f} å­—ç¬¦")

    print(f"\nğŸ’¡ æ¨¡å‹è®¾è®¡å»ºè®®:")
    print(f"  1. max_length è®¾ç½®: 128 (å¯è¦†ç›–å¤§éƒ¨åˆ†æ ·æœ¬)")
    print(f"  2. ç±»åˆ«ä¸å¹³è¡¡å¤„ç†: ä½¿ç”¨ç±»åˆ«æƒé‡æˆ– Focal Loss")
    print(f"  3. éªŒè¯é›†åˆ’åˆ†: å»ºè®®ä½¿ç”¨åˆ†å±‚é‡‡æ · (stratified split)")
    print(f"  4. è¯„ä¼°æŒ‡æ ‡: é™¤äº†å‡†ç¡®ç‡ï¼Œè¿˜åº”å…³æ³¨ F1-scoreã€AUC ç­‰")

    print("\n" + "="*80)
    print("EDA åˆ†æå®Œæˆï¼æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜è‡³ results/ ç›®å½•")
    print("="*80)


def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„EDAæµç¨‹"""
    print("\n" + "ğŸ” "*20)
    print("AFQMC æ•°æ®é›†æ¢ç´¢æ€§åˆ†æ (EDA)")
    print("ğŸ” "*20)

    # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
    dataset_path = project_root / 'dataset'

    # åŠ è½½è®­ç»ƒæ•°æ®
    df = load_train_data(str(dataset_path))

    # 1. æ–‡æœ¬é•¿åº¦åˆ†æ
    df = analyze_text_length(df)

    # 2. æ ‡ç­¾åˆ†å¸ƒåˆ†æ
    analyze_label_distribution(df)

    # 3. ä¸åŒæ ‡ç­¾çš„æ–‡æœ¬ç‰¹å¾
    analyze_text_by_label(df)

    # 4. æ•°æ®è´¨é‡æ£€æŸ¥
    check_data_quality(df)

    # 5. æ ·æœ¬åˆ†æ
    sample_analysis(df)

    # 6. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    generate_eda_report(df)

    print("\n\nâœ… EDA åˆ†æå…¨éƒ¨å®Œæˆï¼")
    print("\nä¸‹ä¸€æ­¥: å¼€å§‹æ„å»ºæ¨¡å‹ ğŸš€")


if __name__ == "__main__":
    # ç¡®ä¿resultsç›®å½•å­˜åœ¨ï¼ˆä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ï¼‰
    results_dir = project_root / 'results'
    results_dir.mkdir(exist_ok=True)

    # åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆç¡®ä¿å›¾ç‰‡ä¿å­˜è·¯å¾„æ­£ç¡®ï¼‰
    os.chdir(project_root)

    main()
