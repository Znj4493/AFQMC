# AFQMCè¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…é¡¹ç›® - åˆ†é˜¶æ®µå­¦ä¹ è®¡åˆ’

## ğŸ¯ è®¡åˆ’æ ¸å¿ƒäº®ç‚¹

æœ¬è®¡åˆ’é’ˆå¯¹2026å¹´AIæ±‚èŒå¸‚åœºç²¾å¿ƒè®¾è®¡ï¼Œé‡ç‚¹çªå‡ºï¼š

1. **è·³è¿‡ä¼ ç»ŸLSTMï¼Œç›´æ¥å­¦ä¹ Transformeræ¶æ„** - ç¬¦åˆå½“å‰è¡Œä¸šè¶‹åŠ¿
2. **ä½¿ç”¨MacBERTä½œä¸ºBaseline** - ä¸­æ–‡ä»»åŠ¡çš„æœ€ä½³é€‰æ‹©ä¹‹ä¸€
3. **LoRAå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹** - æŒæ¡æœ€çƒ­é—¨çš„LLMå¾®è°ƒæŠ€æœ¯
4. **æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯å…¨è¦†ç›–** - AMPã€æ¢¯åº¦ç´¯ç§¯ã€é‡åŒ–ã€æ¢¯åº¦æ£€æŸ¥ç‚¹
5. **å¯¹æ¯”å®éªŒè®¾è®¡** - MacBERT vs LLMï¼Œæ·±å…¥ç†è§£ä¸åŒæ–¹æ¡ˆçš„ä¼˜åŠ£

---

## é¡¹ç›®æ¦‚å†µ

**ä»»åŠ¡ç±»å‹**: ä¸­æ–‡æ–‡æœ¬å¯¹çš„è¯­ä¹‰ç›¸ä¼¼åº¦äºŒåˆ†ç±»ä»»åŠ¡
**æ•°æ®è§„æ¨¡**: è®­ç»ƒé›†32,000æ¡ï¼Œæµ‹è¯•é›†5,000æ¡
**æ•°æ®ç‰¹ç‚¹**:
- æ ‡ç­¾åˆ†å¸ƒä¸å¹³è¡¡ï¼ˆlabel 0: 69.1%, label 1: 30.9%ï¼‰
- é¢†åŸŸï¼šèš‚èšé‡‘æœé‡‘èäº§å“ç›¸å…³é—®é¢˜
- æ ¼å¼ï¼šJSONLï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«text1, text2, labelå­—æ®µï¼‰

**ç¡¬ä»¶ç¯å¢ƒ**: RTX 4060ï¼ˆ8GBæ˜¾å­˜ï¼‰

---

## åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

### é˜¶æ®µä¸€ï¼šç¯å¢ƒæ­å»ºä¸æ•°æ®æ¢ç´¢ï¼ˆ1-2å¤©ï¼‰

**ç›®æ ‡**:
- æ­å»ºæ·±åº¦å­¦ä¹ å¼€å‘ç¯å¢ƒ
- äº†è§£NLPä»»åŠ¡çš„åŸºæœ¬æµç¨‹
- æ¢ç´¢å’Œç†è§£æ•°æ®é›†ç‰¹å¾

**çŸ¥è¯†ç‚¹**:
- PyTorchåŸºç¡€æ¡†æ¶ä»‹ç»
- Transformersåº“çš„ä½œç”¨
- JSONLæ•°æ®æ ¼å¼
- æ–‡æœ¬å¯¹åŒ¹é…ä»»åŠ¡çš„æœ¬è´¨

**å®æ–½æ­¥éª¤**:
1. åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
2. å®‰è£…å¿…è¦çš„åº“ï¼ˆPyTorch, Transformers, pandas, numpyç­‰ï¼‰
3. ç¼–å†™æ•°æ®åŠ è½½è„šæœ¬ï¼ˆ`data_loader.py`ï¼‰
4. è¿›è¡Œæ•°æ®æ¢ç´¢åˆ†æï¼ˆEDAï¼‰ï¼š
   - ç»Ÿè®¡æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ
   - æŸ¥çœ‹ç±»åˆ«åˆ†å¸ƒ
   - åˆ†ææ­£è´Ÿæ ·æœ¬ç‰¹å¾

**äº§å‡ºæ–‡ä»¶**:
- `requirements.txt`: ä¾èµ–åº“åˆ—è¡¨
- `data_loader.py`: æ•°æ®åŠ è½½å·¥å…·
- `eda.ipynb` æˆ– `eda.py`: æ•°æ®æ¢ç´¢è„šæœ¬

---

### é˜¶æ®µäºŒï¼šMacBERT Baselineæ¨¡å‹ï¼ˆ2-3å¤©ï¼‰

**ç›®æ ‡**:
- ç›´æ¥ä¸Šæ‰‹Transformeræ¶æ„
- ç†è§£é¢„è®­ç»ƒæ¨¡å‹çš„å·¥ä½œåŸç†
- å»ºç«‹é«˜è´¨é‡çš„Baseline
- æŒæ¡æ˜¾å­˜ä¼˜åŒ–æŠ€å·§

**çŸ¥è¯†ç‚¹**:
- Transformeræ¶æ„åŸºç¡€
- BERT/MacBERTçš„å·¥ä½œåŸç†
- Tokenizerçš„ä½œç”¨ï¼ˆWordPieceåˆ†è¯ï¼‰
- è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
- æ¢¯åº¦ç´¯ç§¯æŠ€æœ¯
- æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨
- è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€F1-scoreã€AUCï¼‰

**å®æ–½æ­¥éª¤**:
1. ä½¿ç”¨ `hfl/chinese-macbert-base` æ„å»ºæ–‡æœ¬å¯¹åŒ¹é…æ¨¡å‹
   - è¾“å…¥æ ¼å¼ï¼š[CLS] text1 [SEP] text2 [SEP]
   - ä½¿ç”¨[CLS]ä½ç½®çš„è¾“å‡ºè¿›è¡Œåˆ†ç±»
2. ç¼–å†™è®­ç»ƒè„šæœ¬ï¼ˆ`train.py`ï¼‰
   - é›†æˆAMPï¼ˆè‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰
   - å®ç°æ¢¯åº¦ç´¯ç§¯ï¼ˆé€‚åº”8GBæ˜¾å­˜ï¼‰
   - é…ç½®åˆé€‚çš„è¶…å‚æ•°
3. å®ç°éªŒè¯é›†åˆ‡åˆ†å’Œè¯„ä¼°
4. ä¿å­˜æ¨¡å‹checkpoint

**æ˜¾å­˜ä¼˜åŒ–é…ç½®**ï¼ˆé’ˆå¯¹RTX 4060 8GBï¼‰:
- Batch size: 16-32ï¼ˆé…åˆæ¢¯åº¦ç´¯ç§¯ï¼‰
- æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 2-4æ­¥
- æ··åˆç²¾åº¦è®­ç»ƒ: FP16
- æœ€å¤§åºåˆ—é•¿åº¦: 128

**é¢„æœŸæ€§èƒ½**: å‡†ç¡®ç‡çº¦78-83%

**äº§å‡ºæ–‡ä»¶**:
- `model_macbert.py`: MacBERTæ¨¡å‹å®šä¹‰
- `train.py`: è®­ç»ƒä¸»è„šæœ¬ï¼ˆå«AMPå’Œæ¢¯åº¦ç´¯ç§¯ï¼‰
- `utils.py`: å·¥å…·å‡½æ•°ï¼ˆè¯„ä¼°æŒ‡æ ‡ç­‰ï¼‰

---

### é˜¶æ®µä¸‰ï¼šæ¨¡å‹ä¼˜åŒ–ä¸è¿›é˜¶æŠ€å·§ï¼ˆ3-4å¤©ï¼‰

**ç›®æ ‡**:
- å­¦ä¹ æ¨¡å‹ä¼˜åŒ–çš„å¸¸ç”¨æŠ€å·§
- å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
- æå‡MacBERTæ¨¡å‹æ€§èƒ½

**çŸ¥è¯†ç‚¹**:
- å¯¹æŠ—è®­ç»ƒï¼ˆFGMã€PGMï¼‰æå‡é²æ£’æ€§
- æŸå¤±å‡½æ•°ä¼˜åŒ–ï¼ˆFocal Lossã€Label Smoothingï¼‰
- æ•°æ®å¢å¼ºæŠ€æœ¯ï¼ˆEDAã€å›è¯‘ï¼‰
- å­¦ä¹ ç‡warmupå’Œè¡°å‡ç­–ç•¥
- K-foldäº¤å‰éªŒè¯
- æ¨¡å‹é›†æˆæ–¹æ³•

**å®æ–½æ­¥éª¤**:
1. å®ç°é’ˆå¯¹ä¸å¹³è¡¡æ•°æ®çš„ä¼˜åŒ–
   - è°ƒæ•´ç±»åˆ«æƒé‡
   - ä½¿ç”¨Focal Losså¤„ç†ä¸å¹³è¡¡
2. æ·»åŠ å¯¹æŠ—è®­ç»ƒæå‡é²æ£’æ€§ï¼ˆFGMï¼‰
3. å°è¯•ä¸åŒçš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡ç­–ç•¥
   - AdamWä¼˜åŒ–å™¨
   - Warmup + Linear Decay
4. å®ç°ç®€å•çš„æ•°æ®å¢å¼º
5. ï¼ˆå¯é€‰ï¼‰K-foldäº¤å‰éªŒè¯

**é¢„æœŸæ€§èƒ½**: å‡†ç¡®ç‡çº¦83-88%

**äº§å‡ºæ–‡ä»¶**:
- `train_advanced.py`: è¿›é˜¶è®­ç»ƒè„šæœ¬
- `adversarial.py`: å¯¹æŠ—è®­ç»ƒå®ç°
- `data_augmentation.py`: æ•°æ®å¢å¼ºå·¥å…·

---

### é˜¶æ®µå››ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒ - LoRAå®æˆ˜ï¼ˆ3-5å¤©ï¼‰

**ç›®æ ‡**:
- ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„ç‰¹ç‚¹å’Œä¼˜åŠ¿
- æŒæ¡LoRAé«˜æ•ˆå¾®è°ƒæŠ€æœ¯
- å¯¹æ¯”LLMä¸BERTçš„æ•ˆæœå·®å¼‚
- å­¦ä¹ å½“å‰æ±‚èŒå¸‚åœºæœ€çƒ­é—¨çš„æŠ€æœ¯

**çŸ¥è¯†ç‚¹**:
- å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºç¡€æ¦‚å¿µ
- LoRAï¼ˆLow-Rank Adaptationï¼‰åŸç†
- PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰æŠ€æœ¯
- é‡åŒ–æŠ€æœ¯ï¼ˆInt4/Int8ï¼‰é™ä½æ˜¾å­˜éœ€æ±‚
- QLoRAç»“åˆé‡åŒ–å’ŒLoRA
- BitsAndBytesåº“ä½¿ç”¨

**å®æ–½æ­¥éª¤**:
1. é€‰æ‹©è½»é‡çº§å¤§æ¨¡å‹
   - ä¼˜å…ˆï¼š`Qwen/Qwen-1.5-1.8B`ï¼ˆæ˜¾å­˜å‹å¥½ï¼‰
   - å¤‡é€‰ï¼š`Qwen/Qwen-1.5-7B-Chat-Int4`ï¼ˆé‡åŒ–ç‰ˆæœ¬ï¼‰
2. ä½¿ç”¨PEFTåº“å®ç°LoRAå¾®è°ƒ
   - é…ç½®LoRAå‚æ•°ï¼ˆrank, alpha, dropoutï¼‰
   - è®¾ç½®ç›®æ ‡æ¨¡å—ï¼ˆquery, valueå±‚ï¼‰
3. ç¼–å†™LLMè®­ç»ƒè„šæœ¬
   - ä½¿ç”¨4-bité‡åŒ–ï¼ˆQLoRAï¼‰èŠ‚çœæ˜¾å­˜
   - é›†æˆæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰
   - Flash Attention 2åŠ é€Ÿï¼ˆå¦‚æœæ”¯æŒï¼‰
4. å¯¹æ¯”å®éªŒï¼šLLM vs MacBERT
   - æ€§èƒ½å¯¹æ¯”
   - æ¨ç†é€Ÿåº¦å¯¹æ¯”
   - æ˜¾å­˜å ç”¨å¯¹æ¯”

**æ˜¾å­˜ä¼˜åŒ–é…ç½®**ï¼ˆé’ˆå¯¹RTX 4060 8GBï¼‰:
- æ¨¡å‹é‡åŒ–: 4-bitï¼ˆQLoRAï¼‰
- Batch size: 4-8
- æ¢¯åº¦ç´¯ç§¯: 4-8æ­¥
- æ¢¯åº¦æ£€æŸ¥ç‚¹: å¯ç”¨
- LoRA rank: 8-16
- æœ€å¤§åºåˆ—é•¿åº¦: 256-512

**é¢„æœŸæ€§èƒ½**:
- Qwen-1.8B: å‡†ç¡®ç‡çº¦85-90%
- Qwen-7B-Int4: å‡†ç¡®ç‡çº¦88-92%

**äº§å‡ºæ–‡ä»¶**:
- `model_llm.py`: LLMæ¨¡å‹å®šä¹‰
- `train_lora.py`: LoRAå¾®è°ƒè„šæœ¬
- `compare_models.py`: æ¨¡å‹å¯¹æ¯”åˆ†æè„šæœ¬
- `å®éªŒå¯¹æ¯”æŠ¥å‘Š.md`: LLM vs BERTè¯¦ç»†å¯¹æ¯”

---

### é˜¶æ®µäº”ï¼šæ¨ç†ä¸ç»“æœæäº¤ï¼ˆ1-2å¤©ï¼‰

**ç›®æ ‡**:
- å®Œæˆæµ‹è¯•é›†é¢„æµ‹
- ç”Ÿæˆæäº¤æ–‡ä»¶
- å­¦ä¹ æ¨¡å‹éƒ¨ç½²çš„åŸºæœ¬æ¦‚å¿µ

**çŸ¥è¯†ç‚¹**:
- æ¨¡å‹æ¨ç†ä¼˜åŒ–
- Batchæ¨ç†
- ç»“æœåå¤„ç†

**å®æ–½æ­¥éª¤**:
1. ç¼–å†™æ¨ç†è„šæœ¬ï¼ˆ`inference.py`ï¼‰
2. å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
3. ç”Ÿæˆç¬¦åˆæ¯”èµ›è¦æ±‚çš„æäº¤æ–‡ä»¶
4. åˆ†æé”™è¯¯æ ·æœ¬ï¼Œç†è§£æ¨¡å‹å±€é™æ€§

**äº§å‡ºæ–‡ä»¶**:
- `inference.py`: æ¨ç†è„šæœ¬
- `submission.csv` æˆ– `submission.jsonl`: æäº¤æ–‡ä»¶

---

### é˜¶æ®µå…­ï¼šæ€»ç»“ä¸ä¼˜åŒ–ï¼ˆæŒç»­ï¼‰

**ç›®æ ‡**:
- æ•´ç†é¡¹ç›®ä»£ç 
- æ’°å†™æŠ€æœ¯æ–‡æ¡£
- å‡†å¤‡é¢è¯•ææ–™

**å®æ–½æ­¥éª¤**:
1. é‡æ„ä»£ç ï¼Œæé«˜å¯è¯»æ€§
2. ç¼–å†™è¯¦ç»†çš„README.md
3. æ•´ç†å®éªŒè®°å½•å’Œç»“æœå¯¹æ¯”
4. å‡†å¤‡é¡¹ç›®ç­”è¾©ææ–™ï¼ˆå¦‚æœæœ‰ï¼‰
5. æ€»ç»“å­¦åˆ°çš„çŸ¥è¯†ç‚¹ï¼Œå½¢æˆé¢è¯•å‡†å¤‡ææ–™

**äº§å‡ºæ–‡ä»¶**:
- `README.md`: é¡¹ç›®è¯´æ˜æ–‡æ¡£
- `å®éªŒè®°å½•.md`: å„ç§æ–¹æ³•çš„å¯¹æ¯”åˆ†æ
- `é¢è¯•å‡†å¤‡.md`: é¡¹ç›®ç›¸å…³é¢è¯•é—®é¢˜æ•´ç†

---

## å…³é”®æ–‡ä»¶ç»“æ„ï¼ˆæœ€ç»ˆï¼‰

```
AFQMC/
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ train.jsonl          # è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ test.jsonl           # æµ‹è¯•æ•°æ®
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py       # æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ model_macbert.py     # MacBERTæ¨¡å‹
â”‚   â”œâ”€â”€ model_llm.py         # LLMæ¨¡å‹ï¼ˆLoRAï¼‰
â”‚   â”œâ”€â”€ train.py             # è®­ç»ƒè„šæœ¬ï¼ˆå«AMPå’Œæ¢¯åº¦ç´¯ç§¯ï¼‰
â”‚   â”œâ”€â”€ train_advanced.py    # è¿›é˜¶è®­ç»ƒï¼ˆå¯¹æŠ—è®­ç»ƒç­‰ï¼‰
â”‚   â”œâ”€â”€ train_lora.py        # LoRAå¾®è°ƒè„šæœ¬
â”‚   â”œâ”€â”€ inference.py         # æ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ adversarial.py       # å¯¹æŠ—è®­ç»ƒ
â”‚   â””â”€â”€ data_augmentation.py # æ•°æ®å¢å¼º
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py            # é…ç½®æ–‡ä»¶
â”œâ”€â”€ checkpoints/             # æ¨¡å‹ä¿å­˜ç›®å½•
â”‚   â”œâ”€â”€ macbert/
â”‚   â””â”€â”€ qwen-lora/
â”œâ”€â”€ results/                 # ç»“æœä¿å­˜ç›®å½•
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda.ipynb           # æ•°æ®æ¢ç´¢
â”œâ”€â”€ requirements.txt         # ä¾èµ–åº“
â”œâ”€â”€ å®éªŒå¯¹æ¯”æŠ¥å‘Š.md          # LLM vs BERTå¯¹æ¯”
â””â”€â”€ README.md               # é¡¹ç›®è¯´æ˜

```

---

## å­¦ä¹ èµ„æºå»ºè®®

1. **PyTorchåŸºç¡€**: PyTorchå®˜æ–¹æ•™ç¨‹
2. **Transformer**: "Attention is All You Need" è®ºæ–‡
3. **BERT/MacBERT**: "BERT: Pre-training of Deep Bidirectional Transformers" è®ºæ–‡
4. **LoRA**: "LoRA: Low-Rank Adaptation of Large Language Models" è®ºæ–‡
5. **QLoRA**: "QLoRA: Efficient Finetuning of Quantized LLMs" è®ºæ–‡
6. **HuggingFaceåº“**:
   - Transformersæ–‡æ¡£
   - PEFTåº“æ–‡æ¡£
   - BitsAndBytesåº“æ–‡æ¡£
7. **Qwenæ¨¡å‹**: é˜¿é‡Œé€šä¹‰åƒé—®æ¨¡å‹æ–‡æ¡£

---

## æ—¶é—´ä¼°ç®—

- **æ€»è®¡**: çº¦2-3å‘¨
- **æ¯å¤©æŠ•å…¥**: 2-4å°æ—¶
- **å…³é”®é‡Œç¨‹ç¢‘**:
  - ç¬¬1å‘¨æœ«: å®Œæˆé˜¶æ®µä¸€å’Œé˜¶æ®µäºŒï¼ˆMacBERT Baselineï¼Œå‡†ç¡®ç‡78-83%ï¼‰
  - ç¬¬2å‘¨ä¸­: å®Œæˆé˜¶æ®µä¸‰ï¼ˆæ¨¡å‹ä¼˜åŒ–ï¼Œå‡†ç¡®ç‡83-88%ï¼‰
  - ç¬¬2å‘¨æœ«: å®Œæˆé˜¶æ®µå››ï¼ˆLLMå¾®è°ƒï¼Œå‡†ç¡®ç‡85-92%ï¼‰
  - ç¬¬3å‘¨: å¯¹æ¯”åˆ†æã€æäº¤å’Œæ€»ç»“

---

## æˆåŠŸæ ‡å‡†

1. **æŠ€æœ¯ç›®æ ‡**:
   - MacBERTæ¨¡å‹å‡†ç¡®ç‡ > 80%
   - LLMæ¨¡å‹ï¼ˆLoRAå¾®è°ƒï¼‰å‡†ç¡®ç‡ > 85%
   - å®Œæ•´çš„é¡¹ç›®ä»£ç åº“ï¼ˆå«MacBERTå’ŒLLMä¸¤æ¡æŠ€æœ¯è·¯çº¿ï¼‰
   - å¯å¤ç°çš„å®éªŒç»“æœ
   - è¯¦ç»†çš„æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š

2. **å­¦ä¹ ç›®æ ‡**:
   - ç†è§£Transformeræ¶æ„å’ŒBERTåŸç†
   - æŒæ¡PyTorchå’ŒHuggingFace Transformersåº“
   - ç†è§£é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒæµç¨‹
   - æŒæ¡LoRAç­‰é«˜æ•ˆå¾®è°ƒæŠ€æœ¯
   - ç†è§£æ˜¾å­˜ä¼˜åŒ–æŠ€å·§ï¼ˆAMPã€æ¢¯åº¦ç´¯ç§¯ã€é‡åŒ–ç­‰ï¼‰
   - èƒ½å¤Ÿç‹¬ç«‹å®ŒæˆNLPé¡¹ç›®

3. **æ±‚èŒç›®æ ‡**:
   - ä½œä¸ºç®€å†äº®ç‚¹é¡¹ç›®å±•ç¤º
   - èƒ½å¤Ÿè¯¦ç»†è®²è§£MacBERTå’ŒLLMä¸¤ç§æ–¹æ¡ˆ
   - æŒæ¡å½“å‰çƒ­é—¨çš„LoRAå¾®è°ƒæŠ€æœ¯
   - å‡†å¤‡å¥½ç›¸å…³é¢è¯•é—®é¢˜ï¼ˆTransformerã€BERTã€LoRAåŸç†ç­‰ï¼‰

---

## æ ¸å¿ƒæŠ€æœ¯æ ˆ

**åŸºç¡€æ¡†æ¶**:
- PyTorch >= 2.0
- transformers >= 4.36.0
- datasets
- accelerate

**MacBERTç›¸å…³**:
- hfl/chinese-macbert-base

**LLMå¾®è°ƒç›¸å…³**:
- peft (LoRAå®ç°)
- bitsandbytes (é‡åŒ–)
- Qwen/Qwen-1.5-1.8B æˆ– Qwen/Qwen-1.5-7B-Chat

**ä¼˜åŒ–å·¥å…·**:
- torch.cuda.amp (è‡ªåŠ¨æ··åˆç²¾åº¦)
- gradient_checkpointing (æ¢¯åº¦æ£€æŸ¥ç‚¹)
- flash-attention2 (å¯é€‰ï¼Œéœ€GPUæ”¯æŒ)

**æ•°æ®å¤„ç†ä¸è¯„ä¼°**:
- pandas
- numpy
- scikit-learn
- tqdm

---

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³å¼€å§‹**: é˜¶æ®µä¸€ - ç¯å¢ƒæ­å»ºä¸æ•°æ®æ¢ç´¢

å…·ä½“ä»»åŠ¡ï¼š
1. åˆ›å»ºé¡¹ç›®æ–‡ä»¶å¤¹ç»“æ„ï¼ˆsrc/, config/, checkpoints/, results/, notebooks/ï¼‰
2. åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èä½¿ç”¨condaæˆ–venvï¼‰
3. ç¼–å†™ `requirements.txt`ï¼ˆåŒ…å«ä¸Šè¿°æ ¸å¿ƒæŠ€æœ¯æ ˆï¼‰
4. å®‰è£…å¿…è¦çš„åº“
5. ç¼–å†™æ•°æ®åŠ è½½è„šæœ¬ï¼ˆ`data_loader.py`ï¼‰å¹¶æµ‹è¯•è¯»å–æ•°æ®
6. è¿›è¡Œæ•°æ®æ¢ç´¢åˆ†æï¼ˆEDAï¼‰

**é¢„æœŸäº§å‡º**:
- å®Œæ•´çš„å¼€å‘ç¯å¢ƒ
- æ•°æ®ç»Ÿè®¡æŠ¥å‘Šï¼ˆæ–‡æœ¬é•¿åº¦åˆ†å¸ƒã€ç±»åˆ«åˆ†å¸ƒç­‰ï¼‰
- å¯¹æ•°æ®é›†çš„æ·±å…¥ç†è§£
